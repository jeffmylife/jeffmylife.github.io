---
title: "Rambling on AI"
seoTitle: "AI Feels Like Magic"
date: "2025-01-28"
updatedOn: "2025-01-28"
excerpt: "A personal reflection on the evolution of AI, from obscure industry tools to the mainstream magic of ChatGPT."
tags: ["ai", "chatgpt"]
category: "ai"
isPublished: true
featured: true
---

AI existed before ChatGPT, but not for the average consumer. Innovations were happening, of course, yet applications remained in the realm of academics, certain business use-cases, or advanced engineering problems.

These AI applications were mostly B2B or used in companies requiring advanced industry research like biotech, defense, etc. For example, when I worked at a biotech startup, the only solution to scale our technology was to use computer vision techniques in combination with a segmentation model, i.e., a deep neural network.

The exceptions to this were products like Apple's Siri, the Roomba, Netflix's recommendation systems, and random "cool," barely useful apps. Nowadays, few think of AI when thinking of a recommendation system, despite historically being considered "AI." I remember it was a huge deal in the data science world to have recommendation systems be scalable and personalized. Yet, from a consumer standpoint, it was a subtle, even invisible, change in ordering movies.

What's clear is that the business benefits of these systems were tangible: more interactions, longer usage, fewer bounces, etc., but still behind the scenes. The roles of "data scientist" or "machine learning engineer" often revolved around training machine learning models, but for what exactly? Did end-users really feel the magic of what was transpiring?

> AI didn't _feel like AI_ until ChatGPT.

That chat interface wasn't the only reason it felt like AI. There have been those annoying little chats on the bottom of websites for years. In retrospect, they were annoying because they were a terrible way to interact with and find information. If they were sophisticated, they had some sort of semantic text classifier which routed questions to predefined buckets according to user intent. Even still, it was robotic and it felt so.

It wasn't until LMs were sufficiently "intelligent" that these chatbots, or "AI assistants" as sites now say, became something I seek out. My favorite thing is when a website has an AI assistant I can ask questions to. I find myself using these service-specific AI assistants to:

- confirm the product or service fits my exact use case before adopting,
- troubleshoot the system as an existing client or customer before involving a human,
- or even write code I can bring over to my project.

---

Early ChatGPT was simple. It didn't have internet access and often hallucinated facts (sadly, many people to this day have not moved on from this moment). Even so, it was truly amazing to get valid English in response to any question.

For getting answers to "long-tail" questions that Google would just give me SEO-slop for, ChatGPT would do a decent job of understanding what I _mean_ and not what I _say_. That's magic people can feel—it "knows" what I'm thinking, insofar as language communicates more than what is literally said.

This reminds me of the genie who misinterprets wishes:

```yaml
A software developer finds an old lamp and rubs it. Out pops a genie in the visage of Edsger Dijkstra.

"I shall grant you one wish," says Dijkstra, glancing atop the desk to see a framed picture of a childhood home.

The developer thinks carefully and says, "I wish to be outstanding in my field!"

POOF!

The developer finds himself transported to the middle of a wheat field in Kansas, where he grew up.

"Wait, what happened?" he shouts at the sky.

Dijkstra's voice echoes: "You are out standing in your field."
```

```yaml
A software engineer finds an old lamp and rubs it. Out pops a genie in the visage of Edsger Dijkstra.

"I shall grant you one wish," says Dijkstra.

The engineer thinks carefully and says, "I wish to make more money."
The genie nods solemnly. "Understood. Go forth."

The engineer is suddenly compelled to his keyboard and begins typing aggressively at 200 WPM for several minutes. When the trance breaks, he looks up, confused. "What just happened?"

"You have successfully minted 3.2 trillion DijCoins from scratch," the genie replies matter-of-factly. "It features genie-tier, quantum-resistant blockchain enabled by proof-of-spork."

The engineer stares at his screen full of worthless digital currency. "But I meant—"
```

These examples illustrate two distinct failures in communication.

**Using incorrect context**
The first story could be perceived as an auditory failure, hearing "out standing" vs. "outstanding," or as a poor choice in which context to use.
The genie uses the context of _where the developer grew up_ instead of that _he is a programmer_, which tilted the meaning of "outstanding" to be a single-character typo instead of its intended meaning.

**Using context incorrectly**
The second failure was in misunderstanding that money was not just anything that could be used as a currency, but rather something of value—say, dollars. But why did the genie choose to make a shit coin? It used the fact that the wisher was a programmer, of course!

You could argue that the genie was "too literal" in its interpretations, but I don't think that's the correct framing.

> The inability to use context effectively is a disability.
> Intelligence must be a function of context.

To be specific, this is not to say intelligence is _strictly_ a function of context. It's merely my intuition that context and intelligence are very related.
